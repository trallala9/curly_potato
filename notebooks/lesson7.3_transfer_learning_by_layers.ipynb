{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "\n",
    "### Celem jest zapoznanie się z dostępnymi modelami (architekturami).\n",
    "\n",
    "### Modele\n",
    "Jak widać Keras ma trochę dostępnych modeli. Jeśli chcesz poczytać więcej o każdym z nich, poniżej są linki:\n",
    "\n",
    "- [ResNet50: Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "- [VGG16: Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "- [VGG19: Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "- [InceptionResNetV2: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf)\n",
    "- [InceptionV3: Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "- [MobileNet: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "- [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)\n",
    "\n",
    "Weźmiemy na początek jeden z prostszych, ale dość popularny - **VGG16**.\n",
    "\n",
    "*Swoją drogą*, wczytując model, mamy flagę `include_top=False`, która oznacza, że obcinamy warstwę klasyfikatora (czyli `fully-connected layer`). Na początek wczytamy całość (razem z `fully-connected layers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.applications\n",
    "dir(keras.applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaki widać VGG16 ma ponad 138M parametrów (bez `fully-connected` ma 14M i trochę, czyli 10 razy mniej). Domyślnie wszystkie warstwy są \"odmrożone\" (zwróć uwagę na ostatnią linię poprzedniej komórki: `Non-trainable params: 0`).\n",
    "\n",
    "Do warstw można dostać się w taki sposób: `model.layers`. Każda warstwa ma atrybut `trainable`, który zwraca `True` lub `False`. Sprawdźmy to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, layer in enumerate(model.layers):\n",
    "    print(it, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsza warstwa już jest ustawiona na `False`, bo jest to `InputLayer`. Załóżmy, że chcemy trenować (ang. *fine tuning*) tylko ostatni (5. blok), który składa się z czterech warstw (niskopoziomowych):\n",
    "\n",
    "```\n",
    "block5_conv1 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_conv2 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_conv3 (Conv2D)     (None, None, None, 512)   2359808   \n",
    "block5_pool (MaxPool2D)   (None, None, None, 512)   0 \n",
    "```\n",
    "\n",
    "\n",
    "W takiej sytuacji można dla reszty warstw ustawić parametr `trainable` na `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, layer in enumerate(model.layers[:-8]):\n",
    "    layer.trainable = False\n",
    "for it, layer in enumerate(model.layers[-4:]):\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy całość i potem jeszcze `summary` (zwracamy uwagę na ostatnią linię - `Non-trainable params`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, layer in enumerate(model.layers):\n",
    "    print(it, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przypadki zadań \n",
    "W dużym uproszczeniu wszystkie zadania w Deep Learning może klasyfikować wg dwóch wymiarów:\n",
    "![](../images/four_cases.png)\n",
    "\n",
    "A teraz przejdziemy po kolei po każdym przypadku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Danych jest sporo i zadanie bardzo się różni.\n",
    "\n",
    "W takiej sytuacji tak naprawdę uczymy naszą sieć od zera (czyli `ImageNet` trafia do tego przypadku, kiedy mamy miliony zdjęć i zupełnie nowe zadanie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Danych jest dużo, ale zadanie jest podobne.\n",
    "\n",
    "Skoro mamy dużo danych, to możemy uczyć się nawet od zera (przynajmniej nie powinien nam grozić `overfitting`). Z drugiej strony, skoro zadanie jest podobne, to warto wykorzystać już wyliczone wagi. \n",
    "\n",
    "Dlatego strategia jest taka - odmrażamy wszystkie warstwy (`trainable=True`), to jest opcja domyślna. Jedynie jeszcze można przerobić `fully-connected layer` - chyba, że to też nam odpowiada (np. `ImageNet` ma 1000 klas na wyjściu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy, jak to jest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że VGG16 teraz jest pokazany jako warstwa - jest to dość wygodne, w `summary` widać tylko to, co jest ważne (nasze)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Danych jest mało, ale i zadanie jest podobne.\n",
    "\n",
    "W sytuacji gdy danych jest mało lub wcale ich nie ma, to musimy użyć takiego modelu, jaki jest (właśnie to już zrobiliśmy w punkcie `2`) albo zamrozić wszystkie warstwy konwolucyjne i wyrzucić lub odmrozić `fully-connected layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = False #!!!! zwróć uwagę, zamrażamy wszystkie warstwy\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy, jak to jest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W drugim przypadku było `Trainable params: 15,250,250`, a w tym przypadku jest: `Trainable params: 535,562`. Trenujemy tylko `fully-connected layer` i nawet mając mało danych jest szansa, że nie będziemy \"overfitować\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Danych jest mało i zadanie (bardzo) się różni.\n",
    "To jest najgorszy przypadek i niestety dość częsty :) Trzeba sobie jakoś z tym radzić. W tej sytuacji odcięcie tylko `full-connected layer` to raczej za mało - trzeba odmrozić coś więcej (kilka lub więcej warstw konwolucyjnych). Natomiast możemy tutaj zastosować dość sprytne zagranie, które zwykle sprawdza się w praktyce.\n",
    "\n",
    "Na początek zamrażamy wszystkie warstwy konwolucyjne i trenujemy tylko `fully-connected layer`, a następnie odmrażamy wszystko, tylko trenujemy z mniejszym krokiem.\n",
    "\n",
    "### Faza pierwsza\n",
    "Zamrażamy wszystkie warstwy konwolucyjne i trenujemy tylko `fully-connected layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "for layer in base_model.layers[1:]: #input layer ma być na False\n",
    "    layer.trainable = False #!!!! zwróć uwagę, zamrażamy wszystkie warstwy\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    \n",
    "    Flatten(), #<= bridge between conv layers and full connected layers\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "optimizer = Adam(0.001, decay=0.0003) #wartości są przykładowe, zwróć uwagę tylko na rząd\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faza druga\n",
    "\n",
    "Odmrażamy wszystkie lub część warstw z VGG. Swoją drogą, żeby się tam dostać trzeba odwołać się do pierwszej warstwy `model.layers[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[0].layers[10:]: \n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = Adam(0.0001, decay=0.00000001) #wartości są przykładowe, zwróć uwagę tylko na rząd\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "for layer in model.layers[0].layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 7.3.1\n",
    "1. Do której grupy (1,2,3 lub 4) przypiszesz zadanie 6.5 (znaki drogowe)?\n",
    "2. Wyjaśnij, dlaczego tak myślisz. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odpowiedzi 7.3.1\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 7.3.2 (zaawansowane)\n",
    "Spróbuj użyć `VGG` lub `ResNet` dla predykcji znaków drogowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przydatne linki:\n",
    "1. [CS231n: Transfer Learning](http://cs231n.github.io/transfer-learning/)\n",
    "2. [Learning Keras by Implementing the VGG Network From Scratch](https://hackernoon.com/learning-keras-by-implementing-vgg16-from-scratch-d036733f2d5)\n",
    "3. [Experiments on different loss configurations for style transfer](https://towardsdatascience.com/experiments-on-different-loss-configurations-for-style-transfer-7e3147eda55e)\n",
    "4. [Transfer Learning using Keras](https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8)\n",
    "5. [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning)](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
